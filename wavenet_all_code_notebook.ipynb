{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8vEL9lVhTwx"
   },
   "source": [
    "# Homework №5\n",
    "\n",
    "    This homework will be dedicated to the Text-to-Speech(TTS), specifically the neural vocoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbI883ayiDyZ"
   },
   "outputs": [],
   "source": [
    "### COLAB SETUP \n",
    "#!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "#!tar -xf LJSpeech-1.1.tar.bz2\n",
    "#!pip install torchaudio\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KAGGLE SETUP\n",
    "!pip uninstall -y torch\n",
    "!pip uninstall -y torchaudio\n",
    "!pip install torch==1.7.0+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login 6aa2251ef1ea5e572e6a7608c0152db29bd9a294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99mznyKZjpc8",
    "outputId": "39d5e480-6644-45c4-a32e-841f33087a60"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project='wavenet-pytorch')\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh_49BF5hTwx"
   },
   "source": [
    "# Data\n",
    "\n",
    "    In this homework we will use only LJSpeech https://keithito.com/LJ-Speech-Dataset/.\n",
    "\n",
    "    Use the following `featurizer` (his configuration is +- standard for this task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYrV8iw46RDY",
    "outputId": "658f9ba5-d02d-4c82-92f6-dd05cce0d7a9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjVKsIWuhTwx",
    "outputId": "e4babe92-f59e-47ec-935a-f462fb846451"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    sr: int = 22050\n",
    "    win_length: int = 1024\n",
    "    hop_length: int = 256\n",
    "    n_fft: int = 1024\n",
    "    f_min: int = 0\n",
    "    f_max: int = 8000\n",
    "    n_mels: int = 80\n",
    "    power: float = 1.0\n",
    "        \n",
    "    # value of melspectrograms if we fed a silence into `MelSpectrogram`\n",
    "    pad_value: float = -11.5129251\n",
    "\n",
    "\n",
    "class MelSpectrogram(nn.Module):\n",
    "\n",
    "    def __init__(self, config: MelSpectrogramConfig):\n",
    "        super(MelSpectrogram, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=config.sr,\n",
    "            win_length=config.win_length,\n",
    "            hop_length=config.hop_length,\n",
    "            n_fft=config.n_fft,\n",
    "            f_min=config.f_min,\n",
    "            f_max=config.f_max,\n",
    "            n_mels=config.n_mels\n",
    "        )\n",
    "\n",
    "        # The is no way to set power in constructor in 0.5.0 version.\n",
    "        self.mel_spectrogram.spectrogram.power = config.power\n",
    "\n",
    "        # Default `torchaudio` mel basis uses HTK formula. In order to be compatible with WaveGlow\n",
    "        # we decided to use Slaney one instead (as well as `librosa` does by default).\n",
    "        mel_basis = librosa.filters.mel(\n",
    "            sr=config.sr,\n",
    "            n_fft=config.n_fft,\n",
    "            n_mels=config.n_mels,\n",
    "            fmin=config.f_min,\n",
    "            fmax=config.f_max\n",
    "        ).T\n",
    "        self.mel_spectrogram.mel_scale.fb.copy_(torch.tensor(mel_basis)).to(device)\n",
    "    \n",
    "\n",
    "    def forward(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param audio: Expected shape is [B, T]\n",
    "        :return: Shape is [B, n_mels, T']\n",
    "        \"\"\"\n",
    "        \n",
    "        mel = self.mel_spectrogram(audio) \\\n",
    "            .clamp_(min=1e-5) \\\n",
    "            .log_()\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoO_AjM5hTwz"
   },
   "outputs": [],
   "source": [
    "featurizer = MelSpectrogram(MelSpectrogramConfig()).to(device)\n",
    "#wav, sr = torchaudio.load('../dla-ht4/LJSpeech-1.1/wavs/LJ001-0001.wav')\n",
    "#mels = featurizer(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNP1hbKHhTwz"
   },
   "outputs": [],
   "source": [
    "#_, axes = plt.subplots(2, 1, figsize=(15, 7))\n",
    "#axes[0].plot(wav.squeeze())\n",
    "#axes[1].imshow(mels.squeeze())\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mim21rP6hTwz"
   },
   "source": [
    "# Model\n",
    "\n",
    "    1) In this homework you need to implement classical version of WaveNet.\n",
    "        Pay attention on:\n",
    "            1.1) Causal convs. We recommend to implement it via padding.\n",
    "            1.2) \"Condition Network\" which align mel with wav\n",
    "\n",
    "    2) (Bonus) If you have already implemented WaveNet, you can try to implement [Parallel WaveGAN](https://www.dropbox.com/s/bj25vnmkblr9y8v/PWG.pdf?dl=0).\n",
    "        This model is based on WaveNet and GAN.\n",
    "\n",
    "    3) (Bonus) Fast generation of WaveNet. https://arxiv.org/abs/1611.09482.\n",
    "        Don't forget to compare perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbPHJm8thTwz"
   },
   "source": [
    "# Code\n",
    "\n",
    "    1) In this homework you are allowed to use pytorch-lighting.\n",
    "\n",
    "    2) Try to write code more structurally and cleanly!\n",
    "\n",
    "    3) Good logging of experiments save your nerves and time, so we ask you to use W&B.\n",
    "       Log loss, generated and real wavs (in pair, i.e. real wav and wav from correspond mel). \n",
    "       Do not remove the logs until we have checked your work and given you a grade!\n",
    "\n",
    "    4) We also ask you to organize your code in github repo with (Bonus) Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n",
    "\n",
    "    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n",
    "\n",
    "    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some melspec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gExES_m-hTwz"
   },
   "source": [
    "# Report\n",
    "\n",
    "    Finally, you need to write a report in W&B https://www.wandb.com/reports. Add examples of generated mel and audio, compare with GT.\n",
    "    Don't forget to add link to your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSMrgdkjhTwz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7-8UwMQhTwz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QqkSG9bhTwz"
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws1xs2nHhTwz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG0wQHiqhTwz"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWG-3L1ZhTwz"
   },
   "source": [
    "### USEFULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCEM0G3XhTwz"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErU88TEHhTwz"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4J6h5WUhTwz",
    "outputId": "a4c71025-4902-4416-fe1f-71520f41fb41"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdRM_0ztnGH-"
   },
   "outputs": [],
   "source": [
    "# works if size % hop_len == 0\n",
    "\n",
    "def aud_len_from_mel(melspec, win_length=1024, hop_length=256):\n",
    "    return (melspec.size(-1) - 1) * hop_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9Ze-xCehTwz"
   },
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJO22JgahTwz"
   },
   "outputs": [],
   "source": [
    "class MelSpecAudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset containing text and audio.\"\"\"\n",
    "\n",
    "    def __init__(self, root='../input/dlaht4dataset/LJSpeech-1.1/', csv_path='metadata.csv', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root (string): Directory with all the data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.csv = pd.read_csv(root+csv_path, sep='|', header=None)\n",
    "        self.csv = self.csv.drop(columns=[1]).rename(columns={0:'filename', 2:'norm_text'})  # leave only normilized\n",
    "        self.csv = self.csv.dropna().reset_index()\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_name = self.root + 'wavs/' + self.csv.loc[idx, 'filename'] + '.wav'\n",
    "        utt = torchaudio.load(utt_name)[0].squeeze()\n",
    "        \n",
    "        if self.transform:\n",
    "            utt = self.transform(utt)\n",
    "    \n",
    "        sample = {'audio': utt}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kDFZeGjhTwz"
   },
   "outputs": [],
   "source": [
    "def tr_transform(wav, len_sample=15104):\n",
    "    \n",
    "    start = torch.randint(low=0, high=wav.size(0)-len_sample-1, size=(1,)).item()\n",
    "    \n",
    "    return wav[start:start+len_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmLjJy2KhTwz"
   },
   "source": [
    "### LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulhRvZfehTwz",
    "outputId": "76b648dd-0d2c-420c-ab49-afe9add083c9"
   },
   "outputs": [],
   "source": [
    "my_dataset = MelSpecAudioDataset(csv_path='metadata.csv', transform=tr_transform)\n",
    "my_dataset_size = len(my_dataset)\n",
    "print('all train+val samples:', my_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjKymCqUhTwz"
   },
   "outputs": [],
   "source": [
    "train_len = int(my_dataset_size * 0.8)\n",
    "val_len = my_dataset_size - train_len\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdEJztxohTwz"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, \n",
    "                          shuffle=True,\n",
    "                          num_workers=1, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, \n",
    "                        shuffle=True, \n",
    "                        num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz5CdfxphTwz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ok-5ShqXi_fI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ0t76gkhTwz"
   },
   "outputs": [],
   "source": [
    "def field_size(D, L):\n",
    "    res = 0\n",
    "    for i in range(L):\n",
    "        res += 2**(i%D)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size(10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OEs8fuihTw0"
   },
   "source": [
    "### REAL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2T-e_pVhTw0"
   },
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation=1):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "    \n",
    "        self.pad_size = (dilation * (kernel_size - 1))\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, dilation=dilation, padding=0)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad_size, 0), 'constant', 0)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QETq5y9hTw0"
   },
   "outputs": [],
   "source": [
    "class WaveNetLayer(nn.Module):\n",
    "    def __init__(self, input_ch, skip_ch, layer_num):\n",
    "        super(WaveNetLayer, self).__init__()\n",
    "        \n",
    "        self.dil_now = 2**(layer_num % 10)   # 10\n",
    "        \n",
    "        self.W_f = CausalConv1d(input_ch, input_ch, kernel_size=2, dilation=self.dil_now)\n",
    "        self.W_g = CausalConv1d(input_ch, input_ch, kernel_size=2, dilation=self.dil_now)\n",
    "        self.V_f = nn.Conv1d(80, input_ch, kernel_size=1)\n",
    "        self.V_g = nn.Conv1d(80, input_ch, kernel_size=1)\n",
    "        \n",
    "        self.skip_conv = nn.Conv1d(input_ch, skip_ch, kernel_size=1)\n",
    "        self.resid_conv = nn.Conv1d(input_ch, input_ch, kernel_size=1)\n",
    "    \n",
    "        \n",
    "    def forward(self, melspec, wav):\n",
    "        #wav1, wav2 = wav, wav\n",
    "        #mel1, mel2 = melspec, melspec\n",
    "        \n",
    "        z = torch.tanh(self.W_f(wav) + self.V_f(melspec)) \\\n",
    "            * \\\n",
    "            torch.sigmoid(self.W_g(wav) + self.V_g(melspec))\n",
    "        \n",
    "        skip_res = self.skip_conv(z)\n",
    "        \n",
    "        resid_res = self.resid_conv(z)\n",
    "        resid_res = resid_res + wav\n",
    "        \n",
    "        return skip_res, resid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEeU_lK8hTw0"
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, hidden_ch, skip_ch, num_layers, mu):\n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.skip_ch = skip_ch\n",
    "        self.mu = mu\n",
    "        #self.convtr = nn.ConvTranspose1d(in_channels=80, out_channels=80,\n",
    "        #           kernel_size=512,   # 2 * 256 = 2 * hop_len \n",
    "        #           stride=256,        # hop_len \n",
    "        #           padding=256)       # ks // 2)   #\n",
    "        self.embedding = CausalConv1d(1, hidden_ch, kernel_size=512)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(WaveNetLayer(hidden_ch, skip_ch, layer_num=i))\n",
    "        \n",
    "        self.out_conv = nn.Conv1d(skip_ch, mu, kernel_size=1)\n",
    "        self.end_conv = nn.Conv1d(mu, mu, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, melspec, wav):\n",
    "        \n",
    "        melspec = torch.nn.functional.interpolate(melspec, aud_len_from_mel(melspec))[:, :, 1:]   #self.convtr(melspec)[:, :, 1:]  \n",
    "        wav = self.embedding(wav)\n",
    "        \n",
    "        skip_conn_res = torch.zeros((wav.size(0), self.skip_ch, wav.size(-1))).to(wav.device)\n",
    "        for i in range(len(self.layers)):\n",
    "            skip_one, wav = self.layers[i](melspec, wav)\n",
    "            skip_conn_res = skip_conn_res + skip_one\n",
    "            \n",
    "        result_wav = self.end_conv(F.relu(\n",
    "                                          self.out_conv(F.relu(skip_conn_res))\n",
    "                                         ))\n",
    "        \n",
    "        return result_wav\n",
    "\n",
    "\n",
    "     \n",
    "    def inference(self, melspec):\n",
    "        # bs=1\n",
    "        \n",
    "        new_wav_len = aud_len_from_mel(melspec)\n",
    "        melspec = torch.nn.functional.interpolate(melspec, new_wav_len)[:, :, 1:] #self.convtr(melspec)[:, :, 1:]  \n",
    "        \n",
    "        # melspec[:i], wav[:i-1] = сначала 0\n",
    "        whole_melspec = melspec\n",
    "        melspec = melspec[:, :, :1]\n",
    "        wav = torch.zeros((1, 1, 1)).to(melspec.device)\n",
    "        for j in tqdm(range(2, new_wav_len+1)):\n",
    "            # генерим i wav, смотрим на [:i-1] wav (это 0, но это [:1], \n",
    "            # [:i] mel это 1, но это [:2]\n",
    "\n",
    "            new_wav = self.embedding(wav)\n",
    "        \n",
    "            skip_conn_res = torch.zeros((new_wav.size(0), self.skip_ch, new_wav.size(-1))).to(new_wav.device)\n",
    "            for i in range(len(self.layers)):\n",
    "                skip_one, new_wav = self.layers[i](melspec, new_wav)\n",
    "                skip_conn_res = skip_conn_res + skip_one\n",
    "\n",
    "            result_wav = self.end_conv(F.relu(\n",
    "                                              self.out_conv(F.relu(skip_conn_res))\n",
    "                                             ))\n",
    "            result_wav = torch.argmax(result_wav, dim=1)\n",
    "            # обновление\n",
    "            # далее оба -1 дим =2 неверно, поэтому : : -1: но верно ли это?\n",
    "            wav = torch.cat((wav, result_wav.unsqueeze(1)[:, : , -1:]), dim=-1)  #? ? ? ? \n",
    "            melspec = whole_melspec[:, :, :j]\n",
    "        \n",
    "        return wav[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deUNNmHHhTw0"
   },
   "outputs": [],
   "source": [
    "model = WaveNet(hidden_ch=120, skip_ch=240, num_layers=30, mu=256)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u1vxDY5juUs",
    "outputId": "d3d95d68-716e-4cbf-c259-8ca98029583e"
   },
   "outputs": [],
   "source": [
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4vQ1Z4vhTw0",
    "outputId": "5941949c-f666-488f-fd15-816afa1a470b"
   },
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tduP5-pkhTw0",
    "outputId": "0aea1a1c-4f0e-498a-edaf-982427964101"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDzGDJW9hTw1"
   },
   "outputs": [],
   "source": [
    "#checkpoint = torch.load('../input/epoch3/epoch_3', map_location=device)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GGa2ZhahTw1"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASO3R1mPhTw1"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqeKUJhdhTw1"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "#scheduler = StepLR(opt, step_size=500, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlO5tI3phTw1"
   },
   "outputs": [],
   "source": [
    "mu_law_encoder = torchaudio.transforms.MuLawEncoding(quantization_channels=256).to(device)\n",
    "mu_law_decoder = torchaudio.transforms.MuLawDecoding(quantization_channels=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4odlA65m6m5"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBKrmZcs4n7N"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, featurizer, mu_law_encoder):\n",
    "    total_loss = 0\n",
    "    for el in loader:\n",
    "        wav = el['audio'].to(device)\n",
    "        melspec = featurizer(wav)\n",
    "        wav = mu_law_encoder(wav).unsqueeze(1).type(torch.float)  # to device?\n",
    "            \n",
    "        new_wav = model(melspec, wav[:, :, :-1])\n",
    "        new_wav = new_wav.transpose(-1, -2)\n",
    "\n",
    "        ans = wav.type(torch.long)[:, 0, 1:]\n",
    "        loss = F.cross_entropy(new_wav.reshape(-1, 256), ans.reshape(-1))\n",
    "        wandb.log({'val_item_loss':loss.item()})\n",
    "        total_loss = total_loss + loss.item()\n",
    "            \n",
    "        \n",
    "    wandb.log({'val_loss':total_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTJ6pyxWhTw1",
    "outputId": "f99ac541-9559-4867-9f2c-7de2707cba89"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(NUM_EPOCHS)):\n",
    "    for el in train_loader:\n",
    "        wav = el['audio'].to(device)\n",
    "        melspec = featurizer(wav)\n",
    "        wav = mu_law_encoder(wav).unsqueeze(1).type(torch.float)  # to device?\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        new_wav = model(melspec, wav[:, :, :-1])\n",
    "\n",
    "        #print(new_wav.size(), new_wav.transpose(-1, -2).size(),\n",
    "        #      F.log_softmax(new_wav.transpose(-1, -2)).size(), \n",
    "        #      F.log_softmax(new_wav.transpose(-1, -2)).view(-1, 256).size())\n",
    "\n",
    "        new_wav = new_wav.transpose(-1, -2)\n",
    "\n",
    "        ans = wav.type(torch.long)[:, 0, 1:]\n",
    "        loss = F.cross_entropy(new_wav.reshape(-1, 256), ans.reshape(-1))\n",
    "\n",
    "\n",
    "        #print('AFTER' , new_wav.detach().unique().sort())\n",
    "        #print(new_wav.size(), wav.type(torch.long).squeeze().view(-1).size())            \n",
    "        \n",
    "        '''new_wav = F.log_softmax(new_wav, dim=-1).view(-1, 256)\n",
    "        ans = wav.type(torch.long).squeeze()[:, 1:]\n",
    "        loss = F.nll_loss(new_wav, ans.reshape(-1))'''\n",
    "\n",
    "        #print(new_wav, ans)\n",
    "\n",
    "        #print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            \n",
    "        opt.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        wandb.log({'train_loss':loss.item()})\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict()}, 'epoch_'+str(i))\n",
    "    validate(model, val_loader, featurizer, mu_law_encoder)\n",
    "    #print(scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zahpvopg9B1h",
    "outputId": "612d62a2-3e3a-4a4c-d47a-a3e20d18f188"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(model, loader, featurizer, mu_law_encoder):\n",
    "    for el in loader:\n",
    "        wav = el['audio'][:, :4096].to(device)\n",
    "        melspec = featurizer(wav)\n",
    "        wav = mu_law_encoder(wav).unsqueeze(1).type(torch.float)  # to device?\n",
    "\n",
    "        new_wav = model.inference(melspec)\n",
    "\n",
    "        plt.plot(mu_law_decoder(wav.squeeze().detach().cpu()))\n",
    "        plt.show()\n",
    "        plt.plot(mu_law_decoder(new_wav.squeeze().detach().cpu()))\n",
    "        plt.show\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhHNT36DutNr"
   },
   "outputs": [],
   "source": [
    "#plt.plot(mu_law_decoder(wav.squeeze().detach().cpu()))\n",
    "#plt.plot(mu_law_decoder(new_wav.squeeze().detach().cpu()))\n",
    "#plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_65UjJQhTw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxbGdRc1hTw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5jW2lNShTw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nW1e8QWhTw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNG43zSOhTw1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
